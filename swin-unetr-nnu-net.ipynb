{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2542390,"sourceType":"datasetVersion","datasetId":1541666}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Imports\nimport os\nimport tarfile\nfrom glob import glob\nimport shutil\nimport SimpleITK as sitk\nimport pandas as pd\nimport radiomics\nfrom radiomics import featureextractor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import silhouette_score\nimport pickle\nimport json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths\ninput_dir = \"/kaggle/input/brats-2021-task1\"\ntar_path = os.path.join(input_dir, \"BraTS2021_Training_Data.tar\")\npartial_dir = os.path.join(input_dir, \"BraTS2021_Training_Data_part100\")\nimagesTr_dir = \"/kaggle/working/nnUNet_raw/Dataset001_GLI/imagesTr\"\nlabelsTr_dir = \"/kaggle/working/nnUNet_raw/Dataset001_GLI/labelsTr\"\ndataset_json_path = \"/kaggle/working/nnUNet_raw/Dataset001_GLI/dataset.json\"\n\n# Parameters\nmodality_indices = {\"t1\": 0, \"t1ce\": 1, \"t2\": 2, \"flair\": 3}\nn_clusters_range = range(2, 10)\nn_pca_components = 20\nn_folds = 10\nrandom_state = 42","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Unzip first 100 subjects\n# ----------------------\nos.makedirs(partial_dir, exist_ok=True)\nwith tarfile.open(tar_path, \"r:\") as tar:\n    # Identify first 100 top-level subject dirs\n    subject_names = sorted({m.name.split('/')[0] for m in tar.getmembers()})[:100]\n    members_to_extract = [m for m in tar.getmembers() if m.name.split('/')[0] in subject_names]\n    tar.extractall(path=partial_dir, members=members_to_extract)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------\n# Step 2: Organize nnU-Net structure\n# ----------------------\nos.makedirs(imagesTr_dir, exist_ok=True)\nos.makedirs(labelsTr_dir, exist_ok=True)\n\nfor subject in sorted(os.listdir(partial_dir)):\n    pid = subject\n    subj_path = os.path.join(partial_dir, subject)\n    # Copy modalities\n    for mod, idx in modality_indices.items():\n        src = os.path.join(subj_path, f\"{pid}_{mod}.nii.gz\")\n        dst = os.path.join(imagesTr_dir, f\"{pid}_{idx:04d}.nii.gz\")\n        shutil.copy(src, dst)\n    # Copy label\n    shutil.copy(os.path.join(subj_path, f\"{pid}_seg.nii.gz\"),\n                os.path.join(labelsTr_dir, f\"{pid}.nii.gz\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------\n# Step 3: Radiomics Extraction Class\n# ----------------------\nclass RadiomicsExtractor:\n    def __init__(self, images_dir, labels_dir, params=None, modality_idx=3):\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.modality_idx = modality_idx  # e.g., flair (index 3)\n        self.params = params or {\"binWidth\": 25}\n        self.extractor = featureextractor.RadiomicsFeatureExtractor(**self.params)\n\n    def extract(self):\n        features = []\n        for label_path in glob(os.path.join(self.labels_dir, \"*.nii.gz\")):\n            pid = os.path.basename(label_path).replace(\".nii.gz\", \"\")\n            img_path = os.path.join(self.images_dir, f\"{pid}_{self.modality_idx:04d}.nii.gz\")\n            img = sitk.ReadImage(img_path)\n            label = sitk.ReadImage(label_path)\n            result = self.extractor.execute(img, label)\n            result[\"PatientID\"] = pid\n            features.append(result)\n        return pd.DataFrame(features)\n\n# Instantiate and extract\nrad_extractor = RadiomicsExtractor(imagesTr_dir, labelsTr_dir)\ndf_features = rad_extractor.extract()\ndf_features.to_csv(\"gli_radiomics_partial100.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------\n# Step 4: Stratified Splitter Class\n# ----------------------\nclass StratifiedSplitter:\n    def __init__(self, n_clusters_range, n_pca_components, n_folds, random_state=0):\n        self.n_clusters_range = n_clusters_range\n        self.n_pca_components = n_pca_components\n        self.n_folds = n_folds\n        self.random_state = random_state\n        self.scaler = StandardScaler()\n        self.pca = PCA(n_components=self.n_pca_components, random_state=self.random_state)\n        self.kmeans = None\n        self.best_k = None\n\n    def fit_cluster(self, df):\n        X = df.drop(columns=[\"PatientID\"])\n        X_scaled = self.scaler.fit_transform(X)\n        X_pca = self.pca.fit_transform(X_scaled)\n\n        # Grid search for optimal clusters\n        best_score = -1\n        for k in self.n_clusters_range:\n            km = KMeans(n_clusters=k, random_state=self.random_state)\n            labels = km.fit_predict(X_pca)\n            score = silhouette_score(X_pca, labels)\n            if score > best_score:\n                best_score = score\n                self.best_k = k\n\n        # Final clustering\n        self.kmeans = KMeans(n_clusters=self.best_k, random_state=self.random_state)\n        cluster_labels = self.kmeans.fit_predict(X_pca)\n        df[\"Cluster\"] = cluster_labels\n        return df, X_pca\n     def create_folds(self, df):\n        y = df[\"Cluster\"].values\n        pids = df[\"PatientID\"].values\n        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n\n        for fold, (tr, va) in enumerate(skf.split(pids, y)):\n            with open(f\"fold_{fold:02d}_train.txt\", \"w\") as f:\n                f.write(\"\\n\".join(pids[tr]))\n            with open(f\"fold_{fold:02d}_val.txt\",   \"w\") as f:\n                f.write(\"\\n\".join(pids[va]))\n        print(f\"✅ Created {self.n_folds} stratified folds (k={self.best_k} clusters).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------\n# 5) Run extraction + clustering + fold creation\n# clustering + fold splits\nsplitter = StratifiedSplitter(\n    n_clusters_range=range(2,10),\n    n_pca_components=20,\n    n_folds=10,\n    random_state=42\n)\ndf_clustered = splitter.fit_cluster(df_feats)\nsplitter.create_folds(df_clustered)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_list = []\n\nfor fold in range(10):\n    with open(f\"fold_{fold:02d}_train.txt\") as f:\n        train_ids = [pid.strip() for pid in f.readlines()]\n    with open(f\"fold_{fold:02d}_val.txt\") as f:\n        val_ids = [pid.strip() for pid in f.readlines()]\n    \n    split_list.append({\n        'train': train_ids,\n        'val': val_ids\n    })\n\njson_path = \"/kaggle/working/nnUNet_preprocessed/Dataset001_GLI/splits_final.json\"\nwith open(json_path, \"w\") as f:\n    json.dump(split_list, f, indent=4)\n\nprint(f\"✅ Saved custom split file to: {json_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**nnU-Net**","metadata":{}},{"cell_type":"code","source":"#dataset.json file\n# Count number of training cases\n# filenames like <patientID>_0000.nii.gz\nfilenames = glob.glob(os.path.join(imagesTr_dir, \"*.nii.gz\"))\npatient_ids = set(os.path.basename(f).split(\"_\")[0] for f in filenames)\nnum_training = len(patient_ids)\n\n# Construct dataset.json content\ndataset_json = {\n    \"channel_names\": {\n        \"0\": \"T1\",\n        \"1\": \"T1CE\",\n        \"2\": \"T2\",\n        \"3\": \"FLAIR\"\n    },\n    \"labels\": {\n        \"background\": 0,\n        \"whole_tumor\": [1, 2, 3],\n        \"tumor_core\": [2, 3],\n        \"enhancing_tumor\": 3\n    },\n    \"regions_class_order\": [1, 2, 3],\n    \"numTraining\": num_training,  #100\n    \"file_ending\": \".nii.gz\",\n    \"overwrite_image_reader_writer\": \"SimpleITKIO\"\n}\n\n# Ensure directory exists\nos.makedirs(os.path.dirname(dataset_json_path), exist_ok=True)\n\n# Write to file\nwith open(dataset_json_path, \"w\") as f:\n    json.dump(dataset_json, f, indent=4)\n\ndataset_json_path\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"git clone https://github.com/MIC-DKFZ/nnUNet.git\ncd nnUNet\npip install -e .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6.1) Set nnU-Net environment variables\nexport nnUNet_raw_data_base=\"/kaggle/working/nnUNet_raw\"\nexport nnUNet_preprocessed=\"/kaggle/working/nnUNet_preprocessed\"\nexport nnUNet_results=\"/kaggle/working/nnUNet_results\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Install Hidden Layer\npip install --upgrade git+https://github.com/FabianIsensee/hiddenlayer.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plan & preprocess \nnnUNetv2_plan_and_preprocess -d Dataset001_GLI --verify_dataset_integrity","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train all folds for 3d_fullres nnU-Net v2\n\nnnUNetv2_train Dataset001_GLI 3d_fullres all --epochs 100 --npz --val best --c","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}